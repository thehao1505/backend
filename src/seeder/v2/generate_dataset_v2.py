import pandas as pd
import numpy as np
import random
import uuid
import os
from tqdm import tqdm
from collections import defaultdict
import datetime

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
N_USERS = 2000          # ƒê·ªß l·ªõn ƒë·ªÉ test, ƒë·ªß nh·ªè ƒë·ªÉ ch·∫°y nhanh
N_POSTS = 10000         # Kho h√†ng n·ªôi dung
LATENT_DIMS = 768       # [QUAN TR·ªåNG] Kh·ªõp v·ªõi Gemini/OpenAI Embedding
DATA_PATH = './data_synthetic_unified' # Th∆∞ m·ª•c l∆∞u file

# C·∫•u h√¨nh h√†nh vi
AVG_INTERACTIONS = 25   # T∆∞∆°ng t√°c trung b√¨nh/user
FOLLOW_BIAS = 0.6       # 60% t∆∞∆°ng t√°c ƒë·∫øn t·ª´ Follow (CF m·∫°nh ·ªü ƒë√¢y)
NOISE_LEVEL = 0.05      # Nhi·ªÖu th·∫•p ƒë·ªÉ CBF d·ªÖ h·ªçc
POPULARITY_BIAS = 0.1   # Bias ƒë·ªô ph·ªï bi·∫øn

# --- 2. C·∫¶U N·ªêI NG·ªÆ NGHƒ®A (SEMANTIC MAPPING) ---
# Chia 768 chi·ªÅu th√†nh c√°c "v√πng" ch·ªß ƒë·ªÅ.
# N·∫øu Vector c√≥ gi√° tr·ªã cao ·ªü v√πng n√†o -> Sinh text v√πng ƒë√≥.
TOPICS = {
    "TECH": {
        "range": (0, 100), # Chi·ªÅu 0-100
        "keywords": ["NestJS", "React", "TypeScript", "Docker", "Kubernetes", "AI", "Microservices", "Golang", "System Design", "Algorithm"],
        "bios": ["L·∫≠p tr√¨nh vi√™n Backend", "Fullstack Dev", "ƒêam m√™ Open Source", "K·ªπ s∆∞ c·∫ßu n·ªëi", "Y√™u th√≠ch c√¥ng ngh·ªá"]
    },
    "TRAVEL": {
        "range": (100, 200), # Chi·ªÅu 100-200
        "keywords": ["ƒê√† L·∫°t", "Sapa", "H√† Giang", "Bi·ªÉn Nha Trang", "Ph√∫ Qu·ªëc", "C·∫Øm tr·∫°i", "Leo n√∫i", "Homestay view ƒë·∫πp", "SƒÉn m√¢y"],
        "bios": ["Th√≠ch x√™ d·ªãch", "Blogger du l·ªãch", "Ph∆∞·ª£t th·ªß", "Y√™u thi√™n nhi√™n", "S·ªëng ·∫£o"]
    },
    "FOOD": {
        "range": (200, 300), # Chi·ªÅu 200-300
        "keywords": ["Ph·ªü b√≤", "B√∫n ƒë·∫≠u", "Pizza", "Sushi", "Tr√† s·ªØa", "C√† ph√™ tr·ª©ng", "Review ƒë·ªì ƒÉn", "C√¥ng th·ª©c n·∫•u ƒÉn", "Eat clean"],
        "bios": ["T√¢m h·ªìn ƒÉn u·ªëng", "Food Reviewer", "Th√≠ch n·∫•u ƒÉn", "Nghi·ªán tr√† s·ªØa", "ƒê·∫ßu b·∫øp t·∫°i gia"]
    },
    "FINANCE": {
        "range": (300, 400), # Chi·ªÅu 300-400
        "keywords": ["Ch·ª©ng kho√°n", "Bitcoin", "B·∫•t ƒë·ªông s·∫£n", "ƒê·∫ßu t∆∞ v√†ng", "T√†i ch√≠nh c√° nh√¢n", "Startup", "Kinh doanh online", "Passive Income"],
        "bios": ["Nh√† ƒë·∫ßu t∆∞", "Crypto Trader", "Doanh nh√¢n", "Quan t√¢m t√†i ch√≠nh", "Shark Tank fan"]
    },
    "LIFESTYLE": {
        "range": (400, 768), # Chi·ªÅu c√≤n l·∫°i
        "keywords": ["Ch·∫°y b·ªô", "Gym", "Yoga", "ƒê·ªçc s√°ch", "Podcast", "Ch·ªØa l√†nh", "Minimalism", "Th·ªùi trang", "GenZ"],
        "bios": ["S·ªëng t√≠ch c·ª±c", "Y√™u th·ªÉ thao", "M·ªçt s√°ch", "Healthy Lifestyle", "Content Creator"]
    }
}

print(f"üöÄ B·∫Øt ƒë·∫ßu sinh d·ªØ li·ªáu chu·∫©n h√≥a ({N_USERS} users, {N_POSTS} posts)...")
os.makedirs(DATA_PATH, exist_ok=True)

# --- 3. H√ÄM H·ªñ TR·ª¢ ---

def get_random_timestamp():
    start = datetime.datetime.now() - datetime.timedelta(days=365)
    return start + datetime.timedelta(seconds=random.randint(0, 365*24*3600))

def generate_semantic_vectors(n_samples):
    """
    T·∫°o ma tr·∫≠n vector (N x 768).
    Thay v√¨ random ho√†n to√†n, ta 'k√≠ch ho·∫°t' c√°c v√πng ch·ªß ƒë·ªÅ ng·∫´u nhi√™n.
    """
    # Kh·ªüi t·∫°o n·ªÅn nhi·ªÖu th·∫•p (Gaussian noise)
    vectors = np.random.normal(0, 0.05, size=(n_samples, LATENT_DIMS))
    topic_keys = list(TOPICS.keys())
    
    for i in range(n_samples):
        # M·ªói entity (user/post) s·∫Ω m·∫°nh v·ªÅ 1-2 ch·ªß ƒë·ªÅ
        n_topics = random.choices([1, 2], weights=[0.7, 0.3])[0]
        chosen_topics = random.sample(topic_keys, n_topics)
        
        for topic in chosen_topics:
            start, end = TOPICS[topic]["range"]
            # TƒÉng gi√° tr·ªã ·ªü v√πng ch·ªß ƒë·ªÅ n√†y (Signal)
            vectors[i, start:end] += np.random.normal(0.8, 0.2, size=(end-start))
            
    return vectors

def vector_to_text(vector, type="post"):
    """D·ªãch Vector s·ªë h·ªçc sang VƒÉn b·∫£n (Text)."""
    selected_keywords = []
    selected_bios = []
    
    # Qu√©t qua c√°c v√πng ch·ªß ƒë·ªÅ
    for topic, conf in TOPICS.items():
        start, end = conf["range"]
        # T√≠nh ƒëi·ªÉm trung b√¨nh c·ªßa v√πng n√†y
        score = np.mean(vector[start:end])
        
        if score > 0.2: # Ng∆∞·ª°ng k√≠ch ho·∫°t ch·ªß ƒë·ªÅ
            if type == "post":
                selected_keywords.extend(random.sample(conf["keywords"], k=min(2, len(conf["keywords"]))))
            else:
                selected_bios.extend(random.sample(conf["bios"], k=1))
    
    # Fallback n·∫øu vector qu√° y·∫øu (√≠t g·∫∑p)
    if not selected_keywords and type == "post":
        topic = random.choice(list(TOPICS.keys()))
        selected_keywords = random.sample(TOPICS[topic]["keywords"], 2)
    if not selected_bios and type == "user":
        topic = random.choice(list(TOPICS.keys()))
        selected_bios = random.sample(TOPICS[topic]["bios"], 1)

    if type == "post":
        content = f"B√†i vi·∫øt h√¥m nay n√≥i v·ªÅ {', '.join(selected_keywords)}. M·ªçi ng∆∞·ªùi nghƒ© sao? #{selected_keywords[0].replace(' ','')}"
        return content
    else:
        return " | ".join(list(set(selected_bios)))

# --- 4. TH·ª∞C THI ---

# A. T·∫†O USER & VECTOR USER
print("üîπ B∆∞·ªõc 1: Sinh Users & Vectors...")
U_matrix = generate_semantic_vectors(N_USERS) # (N x 768)
users_data = []
user_ids = [str(uuid.uuid4()) for _ in range(N_USERS)]

for i in tqdm(range(N_USERS)):
    bio = vector_to_text(U_matrix[i], type="user")
    users_data.append({
        "id": user_ids[i],
        "username": f"user_{i}",
        "firstName": "User",
        "lastName": str(i),
        "shortDescription": bio, # Text kh·ªõp v·ªõi Vector U[i]
        "email": f"user_{i}@synthetic.com"
    })
pd.DataFrame(users_data).to_csv(f"{DATA_PATH}/users.csv", index=False)


# B. T·∫†O POST & VECTOR POST
print("üîπ B∆∞·ªõc 2: Sinh Posts & Vectors...")
V_matrix = generate_semantic_vectors(N_POSTS) # (M x 768)
posts_data = []
post_ids = [str(uuid.uuid4()) for _ in range(N_POSTS)]

for i in tqdm(range(N_POSTS)):
    content = vector_to_text(V_matrix[i], type="post")
    posts_data.append({
        "id": post_ids[i],
        "authorId": random.choice(user_ids),
        "content": content, # Text kh·ªõp v·ªõi Vector V[i]
        "dwellTimeThreshold": random.randint(3000, 8000),
        "createdAt": get_random_timestamp().isoformat(),
        "parentId": None, 
        "isReply": False
    })
# (B·ªè qua logic t·∫°o reply ph·ª©c t·∫°p ƒë·ªÉ t·∫≠p trung v√†o vector match)
pd.DataFrame(posts_data).to_csv(f"{DATA_PATH}/posts.csv", index=False)


# C. T·∫†O FOLLOW (M·∫†NG X√É H·ªòI)
print("üîπ B∆∞·ªõc 3: Sinh Follow Graph...")
follows_data = []
# User c√≥ vector g·∫ßn nhau th√¨ d·ªÖ follow nhau h∆°n (Homophily)
# ƒê·ªÉ ƒë∆°n gi·∫£n v√† nhanh: D√πng Cosine Similarity tr√™n U_matrix ƒë·ªÉ g·ª£i √Ω follow
# L·∫•y m·∫´u ng·∫´u nhi√™n ƒë·ªÉ t√≠nh to√°n cho nhanh
for i in tqdm(range(N_USERS)):
    # M·ªói user follow kho·∫£ng 20 ng∆∞·ªùi
    # 70% l√† follow ng∆∞·ªùi C√ôNG CH·ª¶ ƒê·ªÄ (High Sim), 30% random
    
    # T√≠nh Sim ƒë∆°n gi·∫£n: Dot product v·ªõi 100 user ng·∫´u nhi√™n
    candidates_idx = np.random.choice(N_USERS, 100)
    scores = U_matrix[i] @ U_matrix[candidates_idx].T
    
    # Top sim
    top_k_idx = candidates_idx[np.argsort(-scores)[:15]] # 15 ng∆∞·ªùi c√πng gu
    random_idx = np.random.choice(N_USERS, 5) # 5 ng∆∞·ªùi random
    
    targets = np.concatenate([top_k_idx, random_idx])
    
    for t_idx in targets:
        if user_ids[t_idx] == user_ids[i]: continue
        follows_data.append({
            "followerId": user_ids[i],
            "followingId": user_ids[t_idx]
        })
pd.DataFrame(follows_data).to_csv(f"{DATA_PATH}/follows.csv", index=False)


# D. T·∫†O T∆Ø∆†NG T√ÅC (INTERACTIONS - GROUND TRUTH)
print("üîπ B∆∞·ªõc 4: T√≠nh Interactions (Ma tr·∫≠n 768D)...")

# 1. T√≠nh Score Matrix = U * V^T
# V√¨ ma tr·∫≠n l·ªõn (2000 * 10000 = 20tr ph·∫ßn t·ª≠), ta t√≠nh t·ª´ng block ho·∫∑c row
# ·ªû ƒë√¢y N=2000 ch·∫°y th·∫≥ng ƒë∆∞·ª£c.
scores_matrix = U_matrix @ V_matrix.T 
# Th√™m Popularity Bias (M·ªôt s·ªë b√†i post c√≥ ƒëi·ªÉm c·ªông cho t·∫•t c·∫£ user)
popularity = np.random.normal(0, POPULARITY_BIAS, size=N_POSTS)
scores_matrix += popularity

train_data = []
test_data = []

# Build follow set for fast lookup
follow_map = defaultdict(set)
for f in follows_data: follow_map[f['followerId']].add(f['followingId'])
post_author_map = {p['id']: p['authorId'] for p in posts_data}

print("   -> Generating actions...")
for i in tqdm(range(N_USERS)):
    uid = user_ids[i]
    u_scores = scores_matrix[i]
    
    # L·∫•y top posts c√≥ ƒëi·ªÉm cao nh·∫•t (H·ª£p gu nh·∫•t)
    # L·∫•y nhi·ªÅu h∆°n c·∫ßn thi·∫øt ƒë·ªÉ l·ªçc
    top_indices = np.argsort(-u_scores)[:100] 
    
    # Ph√¢n lo·∫°i: Follow vs Discovery
    following_posts = []
    discovery_posts = []
    
    for pid_idx in top_indices:
        pid = post_ids[pid_idx]
        author = post_author_map[pid]
        if author == uid: continue
        
        if author in follow_map[uid]:
            following_posts.append(pid)
        else:
            discovery_posts.append(pid)
            
    # Ch·ªçn ra t·∫≠p t∆∞∆°ng t√°c (High Intent)
    n_inter = int(np.random.normal(AVG_INTERACTIONS, 5))
    n_inter = max(5, n_inter)
    
    # Mix theo t·ª∑ l·ªá FOLLOW_BIAS
    n_follow = int(n_inter * FOLLOW_BIAS)
    n_discovery = n_inter - n_follow
    
    final_posts = following_posts[:n_follow] + discovery_posts[:n_discovery]
    random.shuffle(final_posts)
    
    # Chia Train/Test (80/20)
    split = int(len(final_posts) * 0.8)
    train_items = final_posts[:split]
    test_items = final_posts[split:]
    
    # Ghi Train (LIKE/REPLY/VIEW)
    for pid in train_items:
        base_time = get_random_timestamp()
        # 80% LIKE, 20% REPLY
        act_type = "REPLY_POST" if random.random() < 0.2 else "LIKE"
        
        train_data.append({
            "id": str(uuid.uuid4()), "userId": uid, "postId": pid,
            "type": act_type, "createdAt": base_time.isoformat(),
            "dwellTime": None, "searchText": None
        })
        
        # K√®m theo 1 VIEW
        train_data.append({
            "id": str(uuid.uuid4()), "userId": uid, "postId": pid,
            "type": "POST_VIEW", "createdAt": (base_time - datetime.timedelta(seconds=30)).isoformat(),
            "dwellTime": random.randint(5000, 15000), "searchText": None
        })

    # Ghi Test
    for pid in test_items:
        test_data.append({"userId": uid, "postId": pid})
        
    # Th√™m SEARCH h√†nh vi (Quan tr·ªçng cho CBF)
    # N·∫øu user thu·ªôc nh√≥m Tech (d·ª±a tr√™n Vector), cho h·ªç search "React"
    # Ta check l·∫°i vector U[i]
    keywords_to_search = vector_to_text(U_matrix[i], type="user").split(" | ") # L·∫•y l·∫°i keywords t·ª´ vector
    if random.random() < 0.3: # 30% user c√≥ search
        term = random.choice(TOPICS["TECH"]["keywords"]) # Demo l·∫•y random t·ª´ pool t∆∞∆°ng ·ª©ng
        # (Logic l·∫•y ƒë√∫ng topic h∆°i d√†i d√≤ng, ta random ƒë∆°n gi·∫£n trong ph·∫°m vi dataset)
        train_data.append({
            "id": str(uuid.uuid4()), "userId": uid, "postId": None,
            "type": "SEARCH", "createdAt": get_random_timestamp().isoformat(),
            "dwellTime": None, "searchText": "T√¨m ki·∫øm v·ªÅ c√¥ng ngh·ªá" # Placeholder
        })

# S·∫Øp x·∫øp theo th·ªùi gian
df_train = pd.DataFrame(train_data).sort_values("createdAt")
df_train.to_csv(f"{DATA_PATH}/train_interactions.csv", index=False)
pd.DataFrame(test_data).to_csv(f"{DATA_PATH}/test_interactions.csv", index=False)

print(f"\n‚úÖ HO√ÄN T·∫§T! D·ªØ li·ªáu ƒë√£ l∆∞u t·∫°i: {DATA_PATH}")
print("1. Ch·∫°y: python generate_dataset_unified.py")
print("2. S·ª≠a ingest.ts: const DATA_PATH = './data_synthetic_unified'")
print("3. Ch·∫°y ingest -> predict -> evaluate.")

# Normalize cosine formula 
# $$Similarity = \frac{Overlap}{\sqrt{|A| \times |B|}}$$
# Feed ƒë∆∞·ª£c ƒë√°nh gi√°:      cf
# S·ªë user ƒë∆∞·ª£c ƒë√°nh gi√°: 10000
# Mean Precision@10:     19.12%
# Mean Recall@10:        73.22%
# MAP@10:                48.88%

# C√¥ng th·ª©c ch·∫ø bi·∫øn linh tinh
# $$Score = \frac{Overlap}{\sqrt{Overlap \times |A|}} = \sqrt{\frac{Overlap}{|A|}}$$
# Feed ƒë∆∞·ª£c ƒë√°nh gi√°:      cf
# S·ªë user ƒë∆∞·ª£c ƒë√°nh gi√°: 10000
# Mean Precision@10:     19.58%
# Mean Recall@10:        75.09%
# MAP@10:                49.28%