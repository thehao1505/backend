from google import genai
from google.genai import types
import pandas as pd
import numpy as np
import random
import uuid
import os
import datetime
from tqdm import tqdm

# ==============================================================================
# 1. C·∫§U H√åNH (CONFIG)
# ==============================================================================

# [QUAN TR·ªåNG] Thay b·∫±ng API Key c·ªßa b·∫°n
GEMINI_API_KEY = "" 

# C·∫•u h√¨nh D·ªØ li·ªáu
N_USERS = 300           # S·ªë l∆∞·ª£ng user gi·∫£ l·∫≠p
N_POSTS = 1000          # S·ªë l∆∞·ª£ng b√†i vi·∫øt gi·∫£ l·∫≠p
DATA_PATH = './data_synthetic_v4'
EMBEDDING_MODEL = "text-embedding-004" # B·∫ÆT BU·ªòC kh·ªõp v·ªõi embedding.service.ts

# C·∫•u h√¨nh Ch·ªß ƒë·ªÅ (Topics)
# M√¥ t·∫£ n√†y s·∫Ω ƒë∆∞·ª£c g·ª≠i l√™n Google GenAI ƒë·ªÉ l·∫•y vector g·ªëc
TOPICS = {
    "TECH": { 
        "desc": "Software Engineering, Coding, React, NestJS, Docker, Kubernetes, Artificial Intelligence, Machine Learning, Python, System Design.",
        "keywords": ["React", "NestJS", "Docker", "Kubernetes", "AI", "Python", "DevOps"] 
    },
    "TRAVEL": { 
        "desc": "Traveling around the world, Backpacking, Camping in the forest, Hiking mountains, Beautiful beaches, Luxury Resorts, Homestays.",
        "keywords": ["ƒê√† L·∫°t", "Camping", "Bi·ªÉn", "Resort", "Homestay", "Ph∆∞·ª£t", "Leo n√∫i"] 
    },
    "FOOD": { 
        "desc": "Delicious food, Culinary arts, Street food, Fine dining, Sushi, Pizza, Pho noodles, Coffee culture, Cooking recipes.",
        "keywords": ["Ph·ªü", "Sushi", "Coffee", "Street Food", "Pizza", "Dimsum", "Tr√† s·ªØa"] 
    },
    "FINANCE": { 
        "desc": "Financial freedom, Investment strategies, Stock market, Cryptocurrency, Real Estate, Personal Finance, Passive Income.",
        "keywords": ["Crypto", "Stock", "BƒêS", "Gold", "Invest", "Bitcoin", "Ch·ª©ng kho√°n"] 
    },
    "LIFESTYLE": { 
        "desc": "Minimalist lifestyle, Self-improvement, Meditation, Gym workout, Yoga practice, Reading books, Healthy living.",
        "keywords": ["Gym", "Yoga", "Book", "Meditation", "Minimalism", "Healthy", "Workout"] 
    }
}

# Kh·ªüi t·∫°o Client m·ªõi
client = genai.Client(api_key=GEMINI_API_KEY)

# ==============================================================================
# 2. CORE FUNCTIONS
# ==============================================================================

def get_topic_embeddings():
    """
    S·ª≠ d·ª•ng SDK m·ªõi (google-genai) ƒë·ªÉ l·∫•y embedding cho c√°c ch·ªß ƒë·ªÅ.
    """
    topic_keys = list(TOPICS.keys())
    descriptions = [TOPICS[k]["desc"] for k in topic_keys]
    
    print(f"üì° Calling Google GenAI SDK to embed {len(topic_keys)} topics...")
    
    try:
        # G·ªçi API model.embed_content
        response = client.models.embed_content(
            model=EMBEDDING_MODEL,
            contents=descriptions,
            config=types.EmbedContentConfig(
                task_type="RETRIEVAL_DOCUMENT", # T·ªëi ∆∞u vector cho vi·ªác l∆∞u v√†o DB
                title="Topic Descriptions"      # Metadata title (optional)
            )
        )
        
        # Tr√≠ch xu·∫•t vector t·ª´ response object
        # response.embeddings l√† list c√°c Embedding object, m·ªói object c√≥ thu·ªôc t√≠nh .values
        embeddings = np.array([e.values for e in response.embeddings])
        
        return {topic_keys[i]: embeddings[i] for i in range(len(topic_keys))}
        
    except Exception as e:
        print(f"‚ùå Error calling GenAI SDK: {e}")
        print("‚ö†Ô∏è Using Random Vectors fallback (Results will be poor)")
        return {k: np.random.normal(0, 0.1, 768) for k in topic_keys}

def get_random_timestamp():
    start = datetime.datetime.now() - datetime.timedelta(days=90)
    return start + datetime.timedelta(seconds=random.randint(0, 90*24*3600))

# ==============================================================================
# 3. MAIN PROCESS
# ==============================================================================

def main():
    print(f"üöÄ STARTING GENERATION ({N_USERS} Users, {N_POSTS} Posts)")
    os.makedirs(DATA_PATH, exist_ok=True)

    # ---------------------------------------------------------
    # B∆Ø·ªöC 1: L·∫•y Vector G·ªëc t·ª´ Google GenAI
    # ---------------------------------------------------------
    topic_vec_map = get_topic_embeddings()
    topic_keys = list(TOPICS.keys())

    # ---------------------------------------------------------
    # B∆Ø·ªöC 2: Sinh Users (D·ª±a tr√™n Vector Ch·ªß ƒë·ªÅ)
    # ---------------------------------------------------------
    print("üîπ 1. Generating Users...")
    user_ids = [str(uuid.uuid4()) for _ in range(N_USERS)]
    users_data = []
    U_matrix = []

    for i in range(N_USERS):
        # M·ªói user th√≠ch 1 ho·∫∑c 2 ch·ªß ƒë·ªÅ ng·∫´u nhi√™n
        chosen_topics = random.sample(topic_keys, k=random.choices([1, 2], weights=[0.7, 0.3])[0])
        
        # Vector User = Trung b√¨nh c·ªông c√°c Topic Vector + Nhi·ªÖu (Noise)
        base_vec = np.mean([topic_vec_map[t] for t in chosen_topics], axis=0)
        user_vec = base_vec + np.random.normal(0, 0.05, 768) 
        U_matrix.append(user_vec)
        
        # T·∫°o Bio text kh·ªõp v·ªõi ch·ªß ƒë·ªÅ (ƒë·ªÉ Ingest sau n√†y ch·∫°y ƒë√∫ng)
        keywords = []
        for t in chosen_topics:
            keywords.extend(TOPICS[t]["keywords"])
        desc_text = " | ".join(random.sample(keywords, min(4, len(keywords))))

        users_data.append({
            "id": user_ids[i],
            "username": f"user_{i}",
            "firstName": "User", "lastName": str(i),
            "shortDescription": desc_text
        })
    
    pd.DataFrame(users_data).to_csv(f"{DATA_PATH}/users.csv", index=False)

    # ---------------------------------------------------------
    # B∆Ø·ªöC 3: Sinh Posts (D·ª±a tr√™n Vector Ch·ªß ƒë·ªÅ)
    # ---------------------------------------------------------
    print("üîπ 2. Generating Posts...")
    post_ids = [str(uuid.uuid4()) for _ in range(N_POSTS)]
    posts_data = []
    post_author_map = {}
    V_matrix = []

    for i in range(N_POSTS):
        uid = random.choice(user_ids)
        post_author_map[post_ids[i]] = uid
        
        # Post thu·ªôc 1 ch·ªß ƒë·ªÅ
        topic = random.choice(topic_keys)
        
        # Vector Post = Topic Vector + Nhi·ªÖu
        post_vec = topic_vec_map[topic] + np.random.normal(0, 0.05, 768)
        V_matrix.append(post_vec)
        
        # T·∫°o Content text kh·ªõp v·ªõi ch·ªß ƒë·ªÅ
        kws = TOPICS[topic]["keywords"]
        selected_kw = random.sample(kws, 2)
        content = f"Sharing my thoughts on {selected_kw[0]} and {selected_kw[1]}. Truly amazing experience with {TOPICS[topic]['desc']}."

        posts_data.append({
            "id": post_ids[i], "authorId": uid,
            "content": content,
            "dwellTimeThreshold": 3000,
            "createdAt": get_random_timestamp().isoformat()
        })
    
    pd.DataFrame(posts_data).to_csv(f"{DATA_PATH}/posts.csv", index=False)

    # ---------------------------------------------------------
    # B∆Ø·ªöC 4: Sinh Follows (Gi·∫£ l·∫≠p)
    # ---------------------------------------------------------
    print("üîπ 3. Generating Social Graph...")
    follows_data = []
    for i in range(N_USERS):
        targets = random.sample(user_ids, k=random.randint(2, 8))
        for t in targets:
            if t != user_ids[i]:
                follows_data.append({"followerId": user_ids[i], "followingId": t})
    pd.DataFrame(follows_data).to_csv(f"{DATA_PATH}/follows.csv", index=False)

    # ---------------------------------------------------------
    # B∆Ø·ªöC 5: Sinh Interactions (Semantic Matching - Dot Product)
    # ---------------------------------------------------------
    print("üîπ 4. Generating Interactions...")
    
    U_matrix = np.array(U_matrix)
    V_matrix = np.array(V_matrix)
    
    # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa User v√† Post
    # V√¨ c·∫£ hai ƒë·ªÅu sinh t·ª´ vector g·ªëc c·ªßa Gemini, ph√©p nh√¢n n√†y ph·∫£n √°nh ƒë√∫ng "gu"
    scores_matrix = np.dot(U_matrix, V_matrix.T)
    
    interactions_train = []
    
    for i in tqdm(range(N_USERS)):
        uid = user_ids[i]
        u_scores = scores_matrix[i]
        
        # L·∫•y Top 150 b√†i h·ª£p gu nh·∫•t
        top_indices = np.argsort(-u_scores)[:150]
        
        # Lo·∫°i b·ªè b√†i c·ªßa ch√≠nh m√¨nh
        candidate_pool = [post_ids[idx] for idx in top_indices if post_author_map[post_ids[idx]] != uid]
        if not candidate_pool: continue

        # Ch·ªçn ng·∫´u nhi√™n 10-40 b√†i t·ª´ t·∫≠p h·ª£p gu
        n_inter = random.randint(10, 40)
        final_posts = random.sample(candidate_pool, min(n_inter, len(candidate_pool)))
        
        base_time = get_random_timestamp()
        
        for pid in final_posts:
            base_time += datetime.timedelta(minutes=random.randint(10, 120))
            
            # 1. VIEW (Implicit) - Lu√¥n x·∫£y ra
            interactions_train.append({
                "id": str(uuid.uuid4()), "userId": uid, "postId": pid,
                "type": "POST_VIEW", "dwellTime": 10000, "searchText": "",
                "createdAt": base_time.isoformat(),
                "weight": 0.15
            })
            
            # 2. ACTION (Explicit) - X√°c su·∫•t 40%
            if random.random() < 0.4:
                act_type = random.choices(["LIKE", "SHARE", "POST_CLICK"], weights=[0.6, 0.2, 0.2])[0]
                weight = 0.15 if act_type == "LIKE" else (0.35 if act_type == "SHARE" else 0.25)
                
                interactions_train.append({
                    "id": str(uuid.uuid4()), "userId": uid, "postId": pid,
                    "type": act_type, "dwellTime": "", "searchText": "",
                    "createdAt": (base_time + datetime.timedelta(seconds=10)).isoformat(),
                    "weight": weight
                })

    # ---------------------------------------------------------
    # B∆Ø·ªöC 6: Chia Train/Test & L∆∞u file
    # ---------------------------------------------------------
    print("üîπ 5. Splitting Train/Test...")
    df_inter = pd.DataFrame(interactions_train)
    df_inter['createdAt'] = pd.to_datetime(df_inter['createdAt'])
    df_inter = df_inter.sort_values(['userId', 'createdAt'])
    
    train_list, test_list = [], []
    
    for uid, group in df_inter.groupby('userId'):
        if len(group) < 5:
            train_list.append(group)
            continue
        
        split_idx = int(len(group) * 0.8)
        train_list.append(group.iloc[:split_idx])
        
        # Test Set: Ch·ªâ l·∫•y Positive Interactions (Weight >= 0.15)
        test_items = group.iloc[split_idx:]
        test_positives = test_items[test_items['weight'] >= 0.15]
        test_list.append(test_positives)
        
    df_train = pd.concat(train_list)
    df_test = pd.concat(test_list)
    
    df_train.fillna("", inplace=True)
    df_test.fillna("", inplace=True)
    
    df_train.to_csv(f"{DATA_PATH}/train_interactions.csv", index=False)
    df_test[['userId', 'postId']].to_csv(f"{DATA_PATH}/test_interactions.csv", index=False)

    print(f"\n‚úÖ SUCCESS! Data generated in '{DATA_PATH}' using google-genai SDK.")
    print(f"   - Users: {len(users_data)}")
    print(f"   - Posts: {len(posts_data)}")
    print(f"   - Train: {len(df_train)}")
    print(f"   - Test: {len(df_test)}")

if __name__ == "__main__":
    main()