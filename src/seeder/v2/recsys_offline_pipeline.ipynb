{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b531012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG - adjust for scale\n",
    "NUM_USERS = 5000          # recommended: 5k for medium experiments\n",
    "NUM_POSTS = 20000         # recommended: 20k\n",
    "NUM_INTERACTIONS = 150000 # recommended: 150k - 300k depending on memory/time\n",
    "\n",
    "DATA_DIR = './data_synthetic_notebook'  # where CSV outputs will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd79870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and helper functions\n",
    "import os, random, uuid, math\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from faker import Faker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "fake = Faker()\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def precision_at_k(recommended, relevant_set, k):\n",
    "    if k == 0: return 0.0\n",
    "    return sum(1 for x in recommended[:k] if x in relevant_set) / k\n",
    "\n",
    "def recall_at_k(recommended, relevant_set, k):\n",
    "    if not relevant_set: return 0.0\n",
    "    return sum(1 for x in recommended[:k] if x in relevant_set) / len(relevant_set)\n",
    "\n",
    "def average_precision(recommended, relevant_set, k):\n",
    "    if not relevant_set: return 0.0\n",
    "    hits=0; sum_prec=0.0\n",
    "    for i, r in enumerate(recommended[:k], start=1):\n",
    "        if r in relevant_set:\n",
    "            hits += 1\n",
    "            sum_prec += hits / i\n",
    "    return sum_prec / max(1, len(relevant_set))\n",
    "\n",
    "def dcg_at_k(recommended, relevant_set, k):\n",
    "    dcg=0.0\n",
    "    for i, r in enumerate(recommended[:k], start=1):\n",
    "        rel = 1 if r in relevant_set else 0\n",
    "        dcg += (2**rel - 1) / math.log2(i+1)\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(n_rel, k):\n",
    "    idcg=0.0\n",
    "    for i in range(1, min(n_rel,k)+1):\n",
    "        idcg += 1 / math.log2(i+1)\n",
    "    return idcg\n",
    "\n",
    "def ndcg_at_k(recommended, relevant_set, k):\n",
    "    idcg = idcg_at_k(len(relevant_set), k)\n",
    "    if idcg == 0: return 0.0\n",
    "    return dcg_at_k(recommended, relevant_set, k) / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545d702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating users...\n",
      "Building follow graph...\n",
      "Generating posts...\n",
      "Generating interactions... (this can take some time)\n"
     ]
    }
   ],
   "source": [
    "# DATA GENERATION - realistic synthetic dataset\n",
    "TOPICS = ['travel','food','entertainment','tech','education','meme','sports','lifestyle']\n",
    "\n",
    "def generate_users(n_users):\n",
    "    users = []\n",
    "    user_ids = [str(uuid.uuid4()) for _ in range(n_users)]\n",
    "    for uid in user_ids:\n",
    "        r = random.random()\n",
    "        if r < 0.15: user_type = 'power'\n",
    "        elif r < 0.80: user_type = 'normal'\n",
    "        else: user_type = 'lurker'\n",
    "        preferred_topics = random.sample(TOPICS, random.randint(2,3))\n",
    "        users.append({'id':uid, 'name': fake.name(), 'user_type': user_type, 'preferred_topics': preferred_topics, 'following': [], 'followers': []})\n",
    "    return users\n",
    "\n",
    "def build_follow_graph(users):\n",
    "    user_ids = [u['id'] for u in users]\n",
    "    for u in users:\n",
    "        if u['user_type']=='power':\n",
    "            fcount = random.randint(200, 1000)\n",
    "        elif u['user_type']=='normal':\n",
    "            fcount = random.randint(50,200)\n",
    "        else:\n",
    "            fcount = random.randint(5,30)\n",
    "        followings = random.sample([x for x in user_ids if x!=u['id']], min(len(user_ids)-1, fcount))\n",
    "        u['following'] = followings\n",
    "    # populate followers lists\n",
    "    id_to_user = {u['id']:u for u in users}\n",
    "    for u in users:\n",
    "        for fid in u['following']:\n",
    "            id_to_user[fid]['followers'].append(u['id'])\n",
    "\n",
    "def generate_posts(users, n_posts):\n",
    "    posts = []\n",
    "    post_ids = [str(uuid.uuid4()) for _ in range(n_posts)]\n",
    "    for pid in post_ids:\n",
    "        author = random.choice(users)\n",
    "        topic = random.choice(author['preferred_topics'])\n",
    "        created = fake.date_time_between(start_date='-365d', end_date='now')\n",
    "        content = fake.sentence(nb_words=12) + ' ' + random.choice(['#fun','#tips','#review','#howto','#news'])\n",
    "        posts.append({'id':pid, 'author': author['id'], 'topic': topic, 'content': content, 'createdAt': created.isoformat()})\n",
    "    return posts\n",
    "\n",
    "def generate_interactions(users, posts, n_interactions):\n",
    "    ACTIONS = [('LIKE',0.8), ('POST_VIEW',0.12), ('CLICK',0.04), ('SHARE',0.03), ('COMMENT',0.01)]\n",
    "    ACTIONS_list = [a for a,_ in ACTIONS]\n",
    "    post_pop = {p['id']: 1.0 + random.random()*0.5 for p in posts}\n",
    "    viral = set(random.sample([p['id'] for p in posts], max(1,int(len(posts)*0.01))))\n",
    "    for vid in viral: post_pop[vid] += 3.0\n",
    "    \n",
    "    interactions = []\n",
    "    now = datetime.now()\n",
    "    for _ in range(n_interactions):\n",
    "        u = random.choice(users)\n",
    "        if u['following'] and random.random() < 0.7:\n",
    "            candidate = [p for p in posts if p['author'] in u['following']]\n",
    "            if not candidate:\n",
    "                candidate = posts\n",
    "        else:\n",
    "            candidate = posts\n",
    "        weights = [post_pop[p['id']] + (2.0 if p['topic'] in u['preferred_topics'] else 0.0) + 1.0/((now - datetime.fromisoformat(p['createdAt'])).days+1) for p in candidate]\n",
    "        p = random.choices(candidate, weights=weights, k=1)[0]\n",
    "        score = 0.0\n",
    "        if p['topic'] in u['preferred_topics']: score += 1.2\n",
    "        if p['author'] in u['following']: score += 1.0\n",
    "        if p['id'] in viral: score += 2.0\n",
    "        r = random.random()\n",
    "        if score > 3 and r < 0.7:\n",
    "            action = 'LIKE' if random.random() < 0.85 else 'SHARE'\n",
    "        elif score > 2 and r < 0.4:\n",
    "            action = 'LIKE'\n",
    "        else:\n",
    "            action = random.choices(ACTIONS_list, [w for _,w in ACTIONS])[0]\n",
    "        created = fake.date_time_between(start_date='-30d', end_date='now')\n",
    "        interactions.append({'id': str(uuid.uuid4()), 'userId': u['id'], 'postId': p['id'], 'action': action, 'createdAt': created.isoformat()})\n",
    "    return interactions\n",
    "\n",
    "# Run generation\n",
    "print('Generating users...')\n",
    "users = generate_users(NUM_USERS)\n",
    "print('Building follow graph...')\n",
    "build_follow_graph(users)\n",
    "print('Generating posts...')\n",
    "posts = generate_posts(users, NUM_POSTS)\n",
    "print('Generating interactions... (this can take some time)')\n",
    "interactions = generate_interactions(users, posts, NUM_INTERACTIONS)\n",
    "\n",
    "# write CSVs\n",
    "users_df = pd.DataFrame([{k:v for k,v in u.items() if k!='preferred_topics'} for u in users])\n",
    "users_df['preferred_topics'] = users_df.index.map(lambda i: '|'.join(users[i]['preferred_topics']))\n",
    "users_df.to_csv(os.path.join(DATA_DIR, 'users.csv'), index=False)\n",
    "\n",
    "posts_df = pd.DataFrame(posts)\n",
    "posts_df.to_csv(os.path.join(DATA_DIR, 'posts.csv'), index=False)\n",
    "\n",
    "inter_df = pd.DataFrame(interactions)\n",
    "inter_df.to_csv(os.path.join(DATA_DIR, 'interactions_all.csv'), index=False)\n",
    "\n",
    "print('Saved CSVs to', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN/TEST SPLIT - keep last high-intent (LIKE/SHARE/COMMENT) per user as test\n",
    "HIGH_INTENT = set(['LIKE','SHARE','COMMENT'])\n",
    "\n",
    "inter_df = pd.read_csv(os.path.join(DATA_DIR, 'interactions_all.csv'))\n",
    "inter_df['createdAt'] = pd.to_datetime(inter_df['createdAt'])\n",
    "\n",
    "inter_df = inter_df.sort_values(['userId','createdAt'])\n",
    "\n",
    "train_rows = []\n",
    "test_rows = []\n",
    "users_with_test = set()\n",
    "\n",
    "grouped = inter_df.groupby('userId')\n",
    "for uid, g in grouped:\n",
    "    high = g[g['action'].isin(HIGH_INTENT)]\n",
    "    if len(high)>0:\n",
    "        last_high = high.iloc[-1]\n",
    "        test_rows.append(last_high.to_dict())\n",
    "        train = g.drop(index=last_high.name)\n",
    "        train_rows.extend(train.to_dict('records'))\n",
    "        users_with_test.add(uid)\n",
    "    else:\n",
    "        train_rows.extend(g.to_dict('records'))\n",
    "\n",
    "train_df = pd.DataFrame(train_rows)\n",
    "test_df = pd.DataFrame(test_rows)\n",
    "\n",
    "train_df.to_csv(os.path.join(DATA_DIR, 'interactions_train.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(DATA_DIR, 'interactions_test.csv'), index=False)\n",
    "\n",
    "print('Train interactions:', len(train_df))\n",
    "print('Test interactions (high-intent last per user):', len(test_df))\n",
    "print('Users with test:', len(users_with_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mappings and stats\n",
    "users_df = pd.read_csv(os.path.join(DATA_DIR, 'users.csv'))\n",
    "posts_df = pd.read_csv(os.path.join(DATA_DIR, 'posts.csv'))\n",
    "\n",
    "user_ids = users_df['id'].tolist()\n",
    "post_ids = posts_df['id'].tolist()\n",
    "\n",
    "user_map = {uid:i for i,uid in enumerate(user_ids)}\n",
    "post_map = {pid:i for i,pid in enumerate(post_ids)}\n",
    "\n",
    "pop = train_df['postId'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CF: lightweight matrix factorization using TruncatedSVD on implicit-weighted matrix\n",
    "ACTION_WEIGHT = {'LIKE':3.0, 'SHARE':4.0, 'COMMENT':2.5, 'CLICK':1.0, 'POST_VIEW':0.3}\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "rows, cols, vals = [], [], []\n",
    "for _, r in train_df.iterrows():\n",
    "    uidx = user_map.get(r['userId'])\n",
    "    pidx = post_map.get(r['postId'])\n",
    "    if uidx is None or pidx is None: continue\n",
    "    w = ACTION_WEIGHT.get(r['action'], 0.5)\n",
    "    rows.append(pidx); cols.append(uidx); vals.append(w)\n",
    "\n",
    "mat = coo_matrix((vals, (rows, cols)), shape=(len(post_ids), len(user_ids)))\n",
    "print('Interaction matrix shape:', mat.shape, 'nnz=', mat.nnz)\n",
    "\n",
    "svd_dim = 64\n",
    "svd = TruncatedSVD(n_components=min(svd_dim, mat.shape[0]-1), random_state=42)\n",
    "item_factors = svd.fit_transform(mat)  # (n_items, dim)\n",
    "item_factors = normalize(item_factors)\n",
    "user_factors = (mat.T @ item_factors)\n",
    "user_factors = normalize(user_factors)\n",
    "\n",
    "print('CF factors shapes:', item_factors.shape, user_factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBF: TF-IDF on post content\n",
    "tfidf = TfidfVectorizer(max_features=20000, stop_words='english')\n",
    "post_texts = posts_df['content'].fillna('').values.tolist()\n",
    "post_tfidf = tfidf.fit_transform(post_texts)\n",
    "post_tfidf = normalize(post_tfidf)\n",
    "\n",
    "user_profiles = np.zeros((len(user_ids), post_tfidf.shape[1]))\n",
    "counts = np.zeros(len(user_ids))\n",
    "for _, r in train_df[train_df['action']=='LIKE'].iterrows():\n",
    "    uidx = user_map.get(r['userId'])\n",
    "    pidx = post_map.get(r['postId'])\n",
    "    if uidx is None or pidx is None: continue\n",
    "    user_profiles[uidx] += post_tfidf[pidx].toarray().ravel()\n",
    "    counts[uidx] += 1\n",
    "for i in range(len(user_ids)):\n",
    "    if counts[i] > 0:\n",
    "        user_profiles[i] /= counts[i]\n",
    "user_profiles = normalize(user_profiles)\n",
    "print('CBF shapes:', post_tfidf.shape, user_profiles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYBRID: candidate generation and simple logistic re-ranker\n",
    "from heapq import nlargest\n",
    "K_CF = 100; K_CBF = 100; TOP_POP = 200\n",
    "\n",
    "cf_scores = item_factors @ user_factors.T  # (n_items, n_users)\n",
    "\n",
    "def get_cf_topk_for_user(uidx, k):\n",
    "    scores = cf_scores[:, uidx]\n",
    "    top_idx = np.argsort(scores)[-k:][::-1]\n",
    "    return top_idx, scores[top_idx]\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cbf_scores = cosine_similarity(user_profiles, post_tfidf)\n",
    "\n",
    "def get_cbf_topk_for_user(uidx, k):\n",
    "    scores = cbf_scores[uidx]\n",
    "    top_idx = np.argsort(scores)[-k:][::-1]\n",
    "    return top_idx, scores[top_idx]\n",
    "\n",
    "pop_sorted = [pid for pid,_ in sorted(pop.items(), key=lambda x:-x[1])][:TOP_POP]\n",
    "pop_idx = [post_map[pid] for pid in pop_sorted if pid in post_map]\n",
    "\n",
    "LABEL_POS = set(['LIKE','SHARE','COMMENT'])\n",
    "X = []; y = []\n",
    "sampled_users = list(set(train_df['userId'].unique()) & set(user_ids))\n",
    "random.shuffle(sampled_users)\n",
    "max_users = min(2000, len(sampled_users))\n",
    "for uid in tqdm(sampled_users[:max_users]):\n",
    "    uidx = user_map[uid]\n",
    "    cf_idx, cf_sc = get_cf_topk_for_user(uidx, K_CF)\n",
    "    cbf_idx, cbf_sc = get_cbf_topk_for_user(uidx, K_CBF)\n",
    "    candidates = list(dict.fromkeys(list(cf_idx[:50]) + list(cbf_idx[:50]) + pop_idx[:50]))\n",
    "    user_pos = set(train_df[(train_df['userId']==uid) & (train_df['action'].isin(LABEL_POS))]['postId'].tolist())\n",
    "    user_pos_idx = set([post_map[p] for p in user_pos if p in post_map])\n",
    "    for pidx in candidates:\n",
    "        features = [\n",
    "            float(cf_scores[pidx, uidx]),\n",
    "            float(cbf_scores[uidx, pidx]),\n",
    "            float(pop.get(posts_df.loc[pidx,'id'], 0)),\n",
    "            (datetime.now() - datetime.fromisoformat(posts_df.loc[pidx,'createdAt'])).days,\n",
    "            0\n",
    "        ]\n",
    "        X.append(features); y.append(1 if pidx in user_pos_idx else 0)\n",
    "\n",
    "X = np.array(X); y = np.array(y)\n",
    "print('Re-ranker training size:', X.shape, 'pos_ratio=', y.mean())\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X, y)\n",
    "print('Re-ranker trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2581fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION: get top-K per user and compute metrics\n",
    "K = 10\n",
    "users_eval = list(test_df['userId'].unique())\n",
    "precisions = []; recalls = []; aps = []; ndcgs = []\n",
    "\n",
    "for uid in tqdm(users_eval):\n",
    "    uidx = user_map.get(uid)\n",
    "    cf_idx, cf_sc = get_cf_topk_for_user(uidx, K_CF)\n",
    "    cbf_idx, cbf_sc = get_cbf_topk_for_user(uidx, K_CBF)\n",
    "    candidates = list(dict.fromkeys(list(cf_idx[:200]) + list(cbf_idx[:200]) + pop_idx[:200]))\n",
    "    feats = []\n",
    "    for pidx in candidates:\n",
    "        feats.append([cf_scores[pidx, uidx], cbf_scores[uidx, pidx], float(pop.get(posts_df.loc[pidx,'id'], 0)), (datetime.now() - datetime.fromisoformat(posts_df.loc[pidx,'createdAt'])).days, 0])\n",
    "    scores = clf.predict_proba(np.array(feats))[:,1]\n",
    "    ranked_idx = [candidates[i] for i in np.argsort(scores)[::-1][:K]]\n",
    "    rel = set(test_df[test_df['userId']==uid]['postId'].tolist())\n",
    "    rel_idx = set([post_map[p] for p in rel if p in post_map])\n",
    "    precisions.append(precision_at_k(ranked_idx, rel_idx, K))\n",
    "    recalls.append(recall_at_k(ranked_idx, rel_idx, K))\n",
    "    aps.append(average_precision(ranked_idx, rel_idx, K))\n",
    "    ndcgs.append(ndcg_at_k(ranked_idx, rel_idx, K))\n",
    "\n",
    "print('Users evaluated:', len(precisions))\n",
    "print('Mean Precision@10:', np.mean(precisions))\n",
    "print('Mean Recall@10:', np.mean(recalls))\n",
    "print('MAP@10:', np.mean(aps))\n",
    "print('NDCG@10:', np.mean(ndcgs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
